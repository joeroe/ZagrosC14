---
title: "Chronology and regional settlement in the Zagros, 20,000–6000 BP"
output: html_notebook
---

```{r deps}
library("tidyverse")
library("rcarbon")
```

Research compendium containing data and source code for a meta-analysis of chronology and regional settlement in the Epipalaeolithic-Neolithic of the Zagros region (20000-6000 BP).

## Materials & Methods

### Site location data

```{r data-site}
sites <- read_csv("./data/zagros_site.csv")
```

### Radiocarbon data

Import and clean data.
Exclude obvious outliers and dates with unknown errors.
Assign dummy values to dates without a Lab ID.


```{r radiocarbon-data}
read_csv("./data/efertcres_radiocarbon.csv", col_types = cols()) %T>% 
  {message("Imported ", nrow(.), " dates.")} %>%
  filter(!outlier) %>% 
  drop_na(error) %>% 
  mutate(lab_id = if_else(is.na(lab_id), paste0("Unknown-", seq_along(lab_id)), lab_id)) %T>% 
  {message(nrow(.), " dates remaining after cleaning.")} ->
  radiocarbon
```

Normalising the calibrated dates is undesirable because it exaggerates calibration curve artifacts (see Roberts et al. 2017).

```{r calibrate, echo=FALSE}
cal_dates <- calibrate(radiocarbon$cra, radiocarbon$error, ids=radiocarbon$lab_id,
                       normalised = FALSE, calCurves="intcal13")
```

## Results

### Summed probability distribution (SPD)

```{r echo=FALSE}
simple_spd <- spd(cal_dates, timeRange = c(20000, 5000), datenormalised = FALSE,
                  spdnormalised = TRUE)

ggplot() +
  geom_area(data=simple_spd$grid, mapping=aes(x=calBP, y=PrDens)) +
  scale_x_reverse()
```

The simple SPD is definitely bullshit, because certain sites have way more dates than others.

```{r}
ggplot(radiocarbon, aes(x = site_name)) +
  geom_bar(stat = "count") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.3))
```

Binning dates by site reduces this effect.

```{r echo=FALSE}
bsites_spd <- spd(cal_dates, timeRange = c(20000, 5000), datenormalised = FALSE,
                  bins = radiocarbon$site_name, spdnormalised = TRUE)

ggplot() +
  geom_area(data=simple_spd$grid, mapping=aes(x=calBP, y=PrDens), fill="grey") +
  geom_area(data=bsites_spd$grid, mapping=aes(x=calBP, y=PrDens)) +
  scale_x_reverse()
```

Or, we can try Roberts et al.'s (2017) method of creating arbitrary bins consisting of any dates from the same site within a given interval (*h*) in uncalibrated years. However, this method may be sensitive to the choice of *h*.

```{r echo=FALSE}
bintrv <- lapply(c(10, 25, 50, 100, 200, 500), binPrep,
                 sites = radiocarbon$site_name, ages = radiocarbon$cra)
bintrv_spd <- lapply(bintrv, function(b) {
  spd(cal_dates, timeRange = c(20000, 5000), datenormalised = FALSE,
                  bins = b, spdnormalised = TRUE)
})

ggplot() +
  geom_area(data=bsites_spd$grid, mapping=aes(x=calBP, y=PrDens), fill="grey") +
  geom_line(data=bintrv_spd[[1]]$grid, mapping=aes(x=calBP, y=PrDens), colour="red") +
  geom_line(data=bintrv_spd[[2]]$grid, mapping=aes(x=calBP, y=PrDens), colour="orange") +
  geom_line(data=bintrv_spd[[3]]$grid, mapping=aes(x=calBP, y=PrDens), colour="yellow") +
  geom_line(data=bintrv_spd[[4]]$grid, mapping=aes(x=calBP, y=PrDens), colour="green") +
  geom_line(data=bintrv_spd[[5]]$grid, mapping=aes(x=calBP, y=PrDens), colour="blue") +
  geom_line(data=bintrv_spd[[6]]$grid, mapping=aes(x=calBP, y=PrDens), colour="black") +
  scale_x_reverse()
```

The sensitivity analysis shows that for values of *h* between 10 and 500, the resulting SPDs are more similar to each other than to the binned-by-site SPD. Low and high intervals appear to mostly amplify the extremes of the SPD. A middle value of *h*=200 therefore seems best.

```{r echo=FALSE}
bins <- binPrep(radiocarbon$site_name, radiocarbon$cra, 200)
zagros_spd <- spd(cal_dates, timeRange = c(20000, 5000), datenormalised = FALSE,
                  bins = bins, spdnormalised = TRUE)

ggplot() +
  geom_area(data=zagros_spd$grid, mapping=aes(x=calBP, y=PrDens)) +
  scale_x_reverse()
```

#### Simulating a confidence envelope

Using Shennan et al.'s (2013) method, we can use Monte Carlo simulation to account for:
1. Taphonomic effects, by using an exponential null model
2. The effect of the calibration curve (simulated dates are calibrated using the same curve)

The choice of null model here is tricky. 
Uniform is clearly inappropriate because on this period we would expect both long-term population growth and taphonomic loss to be evident.
Shennan argues that exponential (the rcarbon default) should reflect these trends for large (i.e. continental?) regions over long time periods.
But here we are dealing with quite a small region, and for much of the period (c. 18000–12000 BP) the theoretical expectation is
So would a linear or logistic model be better?
Palmisiano et al. use a logistic model for even 12000–2000 BP, justifying it theoretically on the basis of the 'carrying capacity' argument and empirically on the observed shape of their curve.
Nielsen et al. also use it because they're specifically investigating a Neolithic demographic transition, as does the NDT literature in general.
I haven't been able to find any example of linear models being used.

```{r}
# TODO: Look at how Alessio did it
radiocarbon_model <- glm(PrDens ~ calBP, "binomial", zagros_spd$grid,
                         subset = calBP > 8000 & calBP < 18000)

# Use nsim>=1000 for final results
modelTest(cal_dates, radiocarbon$error, nsim = 1000, bins = bins,
          timeRange = c(18000, 8000), model = "custom", predgrid = radiocarbon_model$model,
          datenormalised = FALSE, spdnormalised = TRUE, ncores=8) ->
  radiocarbon_mctest

summary(radiocarbon_mctest)
plot(radiocarbon_mctest)
```
